#LLM'İZİ KULLANICI TERCIHLERİNE GÖRE AYARLAMA

LLM'nizin değerli bir iyileştirme adımı, onu hedef kitlenizin tercihleriyle uyumlu hale getirmektir (aligning it with your audience's preferences). Uygulamanız içinde bir geri bildirim döngüsü (feedback loop) oluşturmalı ve İnsan Geri Bildirimi Veri Kümesi (human feedback dataset) toplamalısınız. Bu sayede LLM'nizi, İnsan Geri Bildirimi ile Pekiştirme Öğrenmesi (Reinforcement Learning with Human Feedback - RLHF) veya daha gelişmiş teknikler olan Doğrudan Tercih Optimizasyonu (Direct Preference Optimization - DPO) gibi yöntemlerle daha da ince ayar yapabilirsiniz. Popüler bir geri bildirim döngüsü, çoğu sohbet robotu arayüzünde bulunan başparmak yukarı/aşağı düğmesidir (thumbs-up/thumbs-down button). Tercih uyumlandırma hakkında daha fazla bilgiyi 6. Bölüm'de okuyabilirsiniz.

İngilizce paragraf:
One valuable refinement step of your LLM is aligning it with your audience’s preferences. You must introduce a feedback loop within your application and gather a human feedback dataset to further fine-tune the LLM with techniques such as Reinforcement Learning with Human Feedback ( RLHF ) or more advanced ones such as Direct Preference Optimization ( DPO ). One popular feedback loop is the thumbs-up/thumbs-down button present in most chatbot interfaces. You can read more on preference alignment in Chapter 6 .

Kodların açıklamaları:
- `One valuable refinement step of your LLM is aligning it with your audience’s preferences.` 
  - Bu satır, LLM'nizin değerli bir iyileştirme adımının, onu hedef kitlenizin tercihleriyle uyumlu hale getirmek olduğunu belirtir.
- `You must introduce a feedback loop within your application...` 
  - Bu satır, uygulamanız içinde bir geri bildirim döngüsü oluşturmanız gerektiğini belirtir.
- `...and gather a human feedback dataset...` 
  - Bu satır, bir İnsan Geri Bildirimi Veri Kümesi toplamanız gerektiğini belirtir.
- `...to further fine-tune the LLM with techniques such as Reinforcement Learning with Human Feedback ( RLHF )...` 
  - Bu satır, LLM'nizi İnsan Geri Bildirimi ile Pekiştirme Öğrenmesi (RLHF) gibi tekniklerle daha da ince ayar yapmanız gerektiğini belirtir.
- `...or more advanced ones such as Direct Preference Optimization ( DPO ).` 
  - Bu satır, Doğrudan Tercih Optimizasyonu (DPO) gibi daha gelişmiş teknikleri kullanabileceğinizi belirtir.
- `One popular feedback loop is the thumbs-up/thumbs-down button present in most chatbot interfaces.` 
  - Bu satır, popüler bir geri bildirim döngüsünün, çoğu sohbet robotu arayüzünde bulunan başparmak yukarı/aşağı düğmesi olduğunu belirtir.
- `You can read more on preference alignment in Chapter 6 .` 
  - Bu satır, tercih uyumlandırma hakkında daha fazla bilgiyi 6. Bölüm'de okuyabileceğinizi belirtir.

---

