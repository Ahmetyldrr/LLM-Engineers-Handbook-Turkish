# Referanslar

Aşağıdaki ingilizce paragrafı bire bir türkçeye çevirisi:

Rafael Rafailov ve diğerleri. “**Direct Preference Optimization: Your Language Model is Secretly a Reward Model** (Doğrudan Tercih Optimizasyonu: Dil Modeliniz Gizlice Bir Ödül Modelidir).” arXiv preprint arXiv:2305.18290, Mayıs 2023.  
Timo Kaufmann ve diğerleri. “**A Survey of Reinforcement Learning from Human Feedback** (İnsan Geribildiriminden Pekiştirmeli Öğrenme Üzerine Bir İnceleme).” arXiv preprint arXiv:2312.14925, Aralık 2023.  
Anthropic. “**GitHub - anthropics/hh-rlhf: Human preference data for “Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback”** (İnsan Tercih Verileri).” github.com, 2022, https://github.com/anthropics/hh-rlhf .  
Nisan Stiennon ve diğerleri. “**Learning to summarize from human feedback** (İnsan geribildiriminden özetlemeyi öğrenme).” arXiv preprint arXiv:2009.01325, Eylül 2020.  
Intel(R) Neural Compressor. “**Supervised Fine-Tuning and Direct Preference Optimization on Intel Gaudi2** (Intel Gaudi2 üzerinde Denetimli İnce Ayar ve Doğrudan Tercih Optimizasyonu).” medium.com, 26 Mart 2024, https://medium.com/intel-analytics-software/the-practice-of-supervised-finetuning-and-direct-preference-optimization-on-habana-gaudi2-a1197d8a3cd3 .  
Argilla. “**GitHub - argilla-io/distilabel**.” github.com , 23 Ağustos 2024, https://github.com/argilla-io/distilabel .  
Databricks. “**Enhancing LLM-as-a-Judge with Grading Notes** (Notlandırma Notlarıyla LLM-as-a-Judge'i Geliştirme).” databricks.com, 22 Temmuz 2024, https://www.databricks.com/blog/enhancing-llm-as-a-judge-with-grading-notes .  
Akrour, Riad & Schoenauer, Marc & Sebag, Michèle. (2011). **Preference-Based Policy Learning** (Tercih Tabanlı Politika Öğrenme). 12-27. 10.1007/978-3-642-23780-5_11.  
Cheng, Weiwei & Fürnkranz, Johannes & Hüllermeier, Eyke & Park, Sang-Hyeun. (2011). **Preference-Based Policy Iteration: Leveraging Preference Learning for Reinforcement Learning** (Tercih Tabanlı Politika İterasyonu: Pekiştirmeli Öğrenme için Tercih Öğrenmesini Kullanma). 312-327. 10.1007/978-3-642-23780-5_30.  
Paul Christiano ve diğerleri. “**Deep reinforcement learning from human preferences** (İnsan tercihlerin-den derin pekiştirmeli öğrenme).” arXiv preprint arXiv:1706.03741, Haziran 2017.  
Long Ouyang ve diğerleri. “**Training language models to follow instructions with human feedback** (Dil modellerini insan geribildirimleriyle talimatları takip etmek üzere eğitmek).” arXiv preprint arXiv:2203.02155, Mart 2022.  
John Schulman ve diğerleri. “**Proximal Policy Optimization Algorithms** (Yakın Politika Optimizasyonu Algoritmaları).” arXiv preprint arXiv:1707.06347, Temmuz 2017.  
unslothai. “**GitHub - unslothai/unsloth: Finetune Llama 3.1, Mistral , Phi & Gemma LLMs 2-5x faster with 80% less memory** (Llama 3.1, Mistral, Phi ve Gemma LLMs'i %80 daha az bellek kullanarak 2-5 kat daha hızlı bir şekilde ince ayar yapın).” github.com, 21 Ağustos 2024, https://github.com/unslothai/unsloth .

Kodların ve referansların ayrıntılı açıklamaları:

1. **Rafael Rafailov et al.. “Direct Preference Optimization: Your Language Model is Secretly a Reward Model.” arXiv preprint arXiv:2305.18290, May 2023.**
   - Yazar: Rafael Rafailov ve diğerleri
   - Başlık: Doğrudan Tercih Optimizasyonu: Dil Modeliniz Gizlice Bir Ödül Modelidir
   - Yayın: arXiv preprint
   - Numara: arXiv:2305.18290
   - Tarih: Mayıs 2023
   - Açıklama: Bu çalışma, dil modellerinin doğrudan tercih optimizasyonu yoluyla ödül modelleri olarak nasıl kullanılabileceğini araştırıyor.

2. **Timo Kaufmann et al.. “A Survey of Reinforcement Learning from Human Feedback.” arXiv preprint arXiv:2312.14925, December 2023.**
   - Yazar: Timo Kaufmann ve diğerleri
   - Başlık: İnsan Geribildiriminden Pekiştirmeli Öğrenme Üzerine Bir İnceleme
   - Yayın: arXiv preprint
   - Numara: arXiv:2312.14925
   - Tarih: Aralık 2023
   - Açıklama: Bu makale, insan geribildiriminden pekiştirmeli öğrenme konusundaki mevcut çalışmaları inceliyor.

3. **Anthropic. “GitHub - anthropics/hh-rlhf: Human preference data for “Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback”.” github.com, 2022**
   - Yazar: Anthropic
   - Başlık: İnsan Tercih Verileri
   - Platform: GitHub
   - Tarih: 2022
   - Açıklama: Bu depo, insan geribildiriminden pekiştirmeli öğrenme kullanarak yararlı ve zararsız bir asistan eğitimi için insan tercih verilerini içerir.

4. **Nisan Stiennon et al.. “Learning to summarize from human feedback.” arXiv preprint arXiv:2009.01325, September 2020.**
   - Yazar: Nisan Stiennon ve diğerleri
   - Başlık: İnsan geribildiriminden özetlemeyi öğrenme
   - Yayın: arXiv preprint
   - Numara: arXiv:2009.01325
   - Tarih: Eylül 2020
   - Açıklama: Bu çalışma, insan geribildirimi kullanarak özetleme becerilerini geliştirmeyi amaçlıyor.

5. **Intel(R) Neural Compressor. “Supervised Fine-Tuning and Direct Preference Optimization on Intel Gaudi2.” medium.com, March 26, 2024**
   - Yazar: Intel(R) Neural Compressor
   - Başlık: Intel Gaudi2 üzerinde Denetimli İnce Ayar ve Doğrudan Tercih Optimizasyonu
   - Platform: Medium
   - Tarih: 26 Mart 2024
   - Açıklama: Bu makale, Intel Gaudi2 üzerinde denetimli ince ayar ve doğrudan tercih optimizasyonunun uygulanmasını tartışıyor.

6. **Argilla. “GitHub - argilla-io/distilabel.” github.com, August 23, 2024**
   - Yazar: Argilla
   - Başlık: GitHub - argilla-io/distilabel
   - Platform: GitHub
   - Tarih: 23 Ağustos 2024
   - Açıklama: Bu depo, distilabel adlı bir proje için kod ve kaynakları içerir.

7. **Databricks. “Enhancing LLM-as-a-Judge with Grading Notes.” databricks.com, July 22, 2024**
   - Yazar: Databricks
   - Başlık: Notlandırma Notlarıyla LLM-as-a-Judge'i Geliştirme
   - Platform: Databricks blog
   - Tarih: 22 Temmuz 2024
   - Açıklama: Bu makale, notlandırma notları kullanarak LLM-as-a-Judge sistemini geliştirme yöntemlerini tartışıyor.

8. **Akrour, Riad & Schoenauer, Marc & Sebag, Michèle. (2011). Preference-Based Policy Learning. 12-27.**
   - Yazar: Akrour, Riad; Schoenauer, Marc; Sebag, Michèle
   - Başlık: Tercih Tabanlı Politika Öğrenme
   - Yayın: Bilimsel makale
   - Sayfa: 12-27
   - DOI: 10.1007/978-3-642-23780-5_11
   - Tarih: 2011
   - Açıklama: Bu çalışma, tercih tabanlı politika öğrenme yöntemlerini tanıtıyor.

9. **Cheng, Weiwei & Fürnkranz, Johannes & Hüllermeier, Eyke & Park, Sang-Hyeun. (2011). Preference-Based Policy Iteration: Leveraging Preference Learning for Reinforcement Learning. 312-327.**
   - Yazar: Cheng, Weiwei; Fürnkranz, Johannes; Hüllermeier, Eyke; Park, Sang-Hyeun
   - Başlık: Tercih Tabanlı Politika İterasyonu: Pekiştirmeli Öğrenme için Tercih Öğrenmesini Kullanma
   - Yayın: Bilimsel makale
   - Sayfa: 312-327
   - DOI: 10.1007/978-3-642-23780-5_30
   - Tarih: 2011
   - Açıklama: Bu makale, pekiştirmeli öğrenme için tercih tabanlı politika iterasyonunu öneriyor.

10. **Paul Christiano et al.. “Deep reinforcement learning from human preferences.” arXiv preprint arXiv:1706.03741, June 2017.**
    - Yazar: Paul Christiano ve diğerleri
    - Başlık: İnsan tercihlerin-den derin pekiştirmeli öğrenme
    - Yayın: arXiv preprint
    - Numara: arXiv:1706.03741
    - Tarih: Haziran 2017
    - Açıklama: Bu çalışma, insan tercihlerini kullanarak derin pekiştirmeli öğrenme yöntemlerini araştırıyor.

11. **Long Ouyang et al.. “Training language models to follow instructions with human feedback.” arXiv preprint arXiv:2203.02155, March 2022.**
    - Yazar: Long Ouyang ve diğerleri
    - Başlık: Dil modellerini insan geribildirimleriyle talimatları takip etmek üzere eğitmek
    - Yayın: arXiv preprint
    - Numara: arXiv:2203.02155
    - Tarih: Mart 2022
    - Açıklama: Bu makale, dil modellerini insan geribildirimi kullanarak talimatları takip etmek üzere eğitme yöntemlerini tartışıyor.

12. **John Schulman et al.. “Proximal Policy Optimization Algorithms.” arXiv preprint arXiv:1707.06347, July 2017.**
    - Yazar: John Schulman ve diğerleri
    - Başlık: Yakın Politika Optimizasyonu Algoritmaları
    - Yayın: arXiv preprint
    - Numara: arXiv:1707.06347
    - Tarih: Temmuz 2017
    - Açıklama: Bu çalışma, yakın politika optimizasyonu algoritmalarını tanıtıyor.

13. **unslothai. “GitHub - unslothai/unsloth: Finetune Llama 3.1, Mistral, Phi & Gemma LLMs 2-5x faster with 80% less memory.” github.com, August 21, 2024**
    - Yazar: unslothai
    - Başlık: Llama 3.1, Mistral, Phi ve Gemma LLMs'i %80 daha az bellek kullanarak 2-5 kat daha hızlı bir şekilde ince ayar yapın
    - Platform: GitHub
    - Tarih: 21 Ağustos 2024
    - Açıklama: Bu depo, büyük dil modellerini daha hızlı ve daha az bellek kullanarak ince ayarlamak için kod sağlar.

---

