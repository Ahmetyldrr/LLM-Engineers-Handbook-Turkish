#Günümüzde Kullanılan LLMs'lerin Çoğunun Mimarisi (Decoder-Only Transformer Architecture)

Günümüzde kullanılan LLMs'lerin (Large Language Models) çoğunun, GPT veya Llama gibi, kodlayıcı (encoder) içermeyen bir Dönüştürücü (Transformer) mimarisi tarafından desteklenmektedir. Kodlayıcı içermeyen mimari, metin oluşturma (text-generation) görevleri için tasarlanmıştır. Bir dizi içindeki bir sonraki kelimeyi önceki kelimelere dayanarak tahmin eder, bu da bağlam içinde uygun metin devamlarının oluşturulmasında etkili olmasını sağlar. Buna karşılık, BERT gibi kodlayıcı tabanlı bir mimari, girdi metnini ayrıntılı gömmeler (embeddings) ile anlamaya ve temsil etmeye odaklanır. Metin sınıflandırma (text classification) ve adlandırılmış varlık tanıma (named entity recognition) gibi kapsamlı bağlam anlamayı gerektiren görevlerde başarılıdır. Son olarak, T5 gibi kodlayıcı-kod çözücü (encoder-decoder) mimarisi, her iki işlevi de birleştirir. Kodlayıcı, girdi metnini işleyerek bağlam açısından zengin bir temsil oluşturur ve daha sonra kod çözücü bu temsili kullanarak çıktı metnini üretir. Bu çift yapı, girdi bağlamını anlamanın ve ilgili bir çıktı oluşturmanın eşit derecede önemli olduğu çeviri ve özetleme gibi dizi-dizi (sequence-to-sequence) görevleri için özellikle güçlüdür. Bu kitapta, LLM alanında hakim olan kodlayıcı içermeyen mimariye odaklanıyoruz.

#Şekil 8.1 – Kodlayıcı İçermeyen Modellerle Çıkarım (Inference) İşlemi
Girdi olarak “I have a dream” veriyoruz ve “of” çıktısını alıyoruz. Şekil 8.1'de gösterildiği gibi, kodlayıcı içermeyen bir model için temel çıkarım işlemi aşağıdaki adımları içerir:
1. Girdi istemini (input prompt) tokenleştirme (tokenizing) ve bir gömme katmanından (embedding layer) ve konumsal kodlamadan (positional encoding) geçirme.
2. Çoklu başlı dikkat mekanizması (multi-head attention mechanism) kullanarak her girdi tokeni için anahtar (key) ve değer (value) çiftlerini hesaplama.
3. Hesaplanan anahtarları ve değerleri kullanarak çıktı tokenlerini sırayla, birer birer oluşturma.

1. Adım ve 2. Adım hesaplamalı olarak pahalı olsa da, yüksek derecede paralel matris çarpımı (matrix multiplication) içerir ve GPU'lar ve TPU'lar gibi hızlandırıcılar üzerinde yüksek donanım kullanımını sağlayabilir. Gerçek zorluk, 3. Adım'daki token oluşturmanın doğası gereği sıralı olmasıdır - bir sonraki tokeni oluşturmak için, önceki tüm tokenlerin oluşturulmuş olması gerekir. Bu, çıktı dizisinin bir token bir seferde büyüdüğü yinelemeli bir sürece yol açar ve donanımın paralel hesaplama yeteneklerinden yararlanamaz. Bu darboğazı ele almak, çıkarım optimizasyonunun temel odak noktalarından biridir.

Bu bölümde, çıkarım hızını artırmak ve Video Rastgele Erişimli Bellek (VRAM) kullanımını azaltmak için yaygın olarak kullanılan çeşitli optimizasyon stratejilerini ayrıntılı olarak ele alacağız, örneğin statik KV önbelleği (static KV cache), sürekli toplu işlemleme (continuous batching), spekülatif kod çözme (speculative decoding) ve optimize edilmiş dikkat mekanizmaları (optimized attention mechanisms) gibi.

Kodların ayrıntılı açıklamaları:
- Tokenleştirme (Tokenizing): Metni token adı verilen alt birimlere ayırma işlemidir.
- Gömme Katmanı (Embedding Layer): Tokenleri vektör uzayında temsil eden katmandır.
- Konumsal Kodlama (Positional Encoding): Tokenlerin sırasını modellemek için kullanılan tekniktir.
- Çoklu Başlı Dikkat Mekanizması (Multi-Head Attention Mechanism): Modelin girdi dizisinin farklı bölümlerine odaklanmasını sağlayan mekanizmadır.
- Anahtar (Key) ve Değer (Value) Çiftleri: Dikkat mekanizması tarafından kullanılan, girdi tokenlerinin temsilidir.
- KV Önbelleği (KV Cache): Anahtar ve değer çiftlerini önbelleğe alma işlemidir, böylece her adımda yeniden hesaplanmazlar.
- Sürekli Toplu İşlemleme (Continuous Batching): Çıkarım işleminin toplu olarak işlenmesini sağlayan tekniktir.
- Spekülatif Kod Çözme (Speculative Decoding): Çıkarım hızını artırmak için kullanılan bir tekniktir.
- Optimize Edilmiş Dikkat Mekanizmaları (Optimized Attention Mechanisms): Dikkat mekanizmasının optimize edilmiş halidir.

İngilizce paragraf:
```
Most of the LLMs used nowadays, like GPT or Llama, are powered by a decoder-only Transformer architecture. The decoder-only architecture is designed for text-generation tasks. It predicts the next word in a sequence based on preceding words, making it effective for generating contextually appropriate text continuations. In contrast, an encoder-only architecture, like BERT, focuses on understanding and representing the input text with detailed embeddings. It excels in tasks that require comprehensive context understanding, such as text classification and named entity recognition. Finally, the encoder-decoder architecture, like T5, combines both functionalities. The encoder processes the input text to generate a context-rich representation, which the decoder then uses to produce the output text. This dual structure is particularly powerful for sequence-to-sequence tasks like translation and summarization, where understanding the input context and generating a relevant output are equally important. In this book, we only focus on the decoder-only architecture, which dominates the LLM field. Figure 8.1 – Inference process with decoder-only models. We provide “I have a dream” as input and obtain “of” as output. As shown in Figure 8.1 , the basic inference process for a decoder-only model involves: Tokenizing the input prompt and passing it through an embedding layer and positional encoding. Computing key and value pairs for each input token using the multi-head attention mechanism. Generating output tokens sequentially, one at a time, using the computed keys and values. While Steps 1 and 2 are computationally expensive, they consist of highly parallelizable matrix multiplication that can achieve high hardware utilization on accelerators like GPUs and TPUs. The real challenge is that the token generation in Step 3 is inherently sequential – to generate the next token, you need to have generated all previous tokens. This leads to an iterative process where the output sequence is grown one token at a time, failing to leverage the parallel computing capabilities of the hardware. Addressing this bottleneck is one of the core focuses of inference optimization. In this section, we will detail several optimization strategies that are commonly used to speed up inference and reduce Video Random-Access Memory ( VRAM ) usage, such as implementing a (static) KV cache, continuous batching, speculative decoding, and optimized attention mechanisms.
```

---

