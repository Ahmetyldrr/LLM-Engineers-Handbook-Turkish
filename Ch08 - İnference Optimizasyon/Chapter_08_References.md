# Hugging Face, Text Generation Inference, https://github.com/huggingface/text-generation-inference , 2022 
# W. Kwon , Z. Li , S. Zhuang , Y. Sheng , L. Zheng , C.H. Yu , J.E. Gonzalez , H. Zhang , I. Stoica , Efficient Memory Management for Large Language Model Serving with PagedAttention , 2023 
# Nvidia, TensorRT-LLM , https://github.com/NVIDIA/TensorRT-LLM , 2023 
# Y. Leviathan, M. Kalman, Y. Matias, Fast Inference from Transformers via Speculative Decoding, 2023 
# T. Cai, Y. Li, Z. Geng, H. Peng, J.D. Lee, D. Chen, T. Dao, Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads, 2024 
# W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C.H. Yu, J.E. Gonzalez, H. Zhang, I. Stoica, Efficient Memory Management for Large Language Model Serving with PagedAttention, 2023 
# R.Y. Aminabadi, S. Rajbhandari, M. Zhang, A.A. Awan, C. Li, D. Li, E. Zheng, J. Rasley, S. Smith, O. Ruwase, Y. He, DeepSpeed Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale, 2022 
# Y. Huang, Y. Cheng, A. Bapna, O. Firat, M.X. Chen, D. Chen, H. Lee, J. Ngiam, Q.V. Le, Y. Wu, Z. Chen, GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism, 2019 
# K. James Reed, PiPPy: Pipeline Parallelism for PyTorch , https://github.com/pytorch/PiPPy , 2022 
# M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, B. Catanzaro, Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism, 2020 
# Verma and Vaidya, Mastering LLM Techniques: Inference Optimization, NVIDIA Developer Technical Blog , https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/ , 2023 
# T. Dettmers, M. Lewis, Y. Belkada, L. Zettlemoyer, LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale, 2022 
# G. Gerganov, llama.cpp , https://github.com/ggerganov/llama.cpp , 2023 
# E. Frantar, S. Ashkboos, T. Hoefler, D. Alistarh, GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers, 2023 
# Tuboderp, exllamav2 , https://github.com/turboderp/exllamav2 , 2023 
# J. Lin, J. Tang, H. Tang, S. Yang, W.-M. Chen, W.-C. Wang, G. Xiao, X. Dang, C. Gan, S. Han, AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration, 2024 .

İngilizce paragrafın birebir Türkçe çevirisi:
# Hugging Face, Metin Üretimi Çıkarımı (Text Generation Inference), https://github.com/huggingface/text-generation-inference , 2022 
# W. Kwon , Z. Li , S. Zhuang , Y. Sheng , L. Zheng , C.H. Yu , J.E. Gonzalez , H. Zhang , I. Stoica , Büyük Dil Modeli Sunumu için Sayfalandırılmış Dikkat (PagedAttention) ile Etkin Bellek Yönetimi (Efficient Memory Management), 2023 
# Nvidia, TensorRT-LLM , https://github.com/NVIDIA/TensorRT-LLM , 2023 
# Y. Leviathan, M. Kalman, Y. Matias, Spekülatif Kod Çözme (Speculative Decoding) Yoluyla Transformatörlerden Hızlı Çıkarım, 2023 
# T. Cai, Y. Li, Z. Geng, H. Peng, J.D. Lee, D. Chen, T. Dao, Medusa: Çoklu Kod Çözme Başlıkları (Multiple Decoding Heads) ile Basit Büyük Dil Modeli Çıkarım Hızlandırma Çerçevesi, 2024 
# W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C.H. Yu, J.E. Gonzalez, H. Zhang, I. Stoica, Büyük Dil Modeli Sunumu için Sayfalandırılmış Dikkat ile Etkin Bellek Yönetimi, 2023 
# R.Y. Aminabadi, S. Rajbhandari, M. Zhang, A.A. Awan, C. Li, D. Li, E. Zheng, J. Rasley, S. Smith, O. Ruwase, Y. He, DeepSpeed Çıkarımı: Transformatör Modellerinin Eşi Görülmemiş Ölçekte Etkin Çıkarımını Sağlama, 2022 
# Y. Huang, Y. Cheng, A. Bapna, O. Firat, M.X. Chen, D. Chen, H. Lee, J. Ngiam, Q.V. Le, Y. Wu, Z. Chen, GPipe: Boru Hattı Paralelliği (Pipeline Parallelism) Kullanarak Dev Sinir Ağlarının Etkin Eğitimi, 2019 
# K. James Reed, PiPPy: PyTorch için Boru Hattı Paralelliği , https://github.com/pytorch/PiPPy , 2022 
# M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, B. Catanzaro, Megatron-LM: Model Paralelliği (Model Parallelism) Kullanarak Çoklu-Milyar Parametreli Dil Modellerinin Eğitimi, 2020 
# Verma ve Vaidya, Büyük Dil Modeli Tekniklerinde Ustalaşma: Çıkarım Optimizasyonu, NVIDIA Geliştirici Teknik Blogu , https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/ , 2023 
# T. Dettmers, M. Lewis, Y. Belkada, L. Zettlemoyer, LLM.int8(): Ölçekte Transformatörler için 8-bit Matris Çarpımı, 2022 
# G. Gerganov, llama.cpp , https://github.com/ggerganov/llama.cpp , 2023 
# E. Frantar, S. Ashkboos, T. Hoefler, D. Alistarh, GPTQ: Üretken Önceden Eğitilmiş Transformatörler için Doğru Eğitim Sonrası Kuantalama (Post-Training Quantization), 2023 
# Tuboderp, exllamav2 , https://github.com/turboderp/exllamav2 , 2023 
# J. Lin, J. Tang, H. Tang, S. Yang, W.-M. Chen, W.-C. Wang, G. Xiao, X. Dang, C. Gan, S. Han, AWQ: Büyük Dil Modeli Sıkıştırma ve Hızlandırma için Aktivasyon-Duyarlı Ağırlık Kuantalama (Activation-aware Weight Quantization), 2024 .

Kodların ayrıntılı açıklamaları:

1. `# Hugging Face, Text Generation Inference, https://github.com/huggingface/text-generation-inference , 2022`
   - Hugging Face tarafından geliştirilen Metin Üretimi Çıkarımı (Text Generation Inference) projesine atıfta bulunmaktadır.
   - Bu proje, büyük dil modellerinin metin üretme işlemlerini optimize etmeye yöneliktir.

2. `# W. Kwon , Z. Li , S. Zhuang , Y. Sheng , L. Zheng , C.H. Yu , J.E. Gonzalez , H. Zhang , I. Stoica , Efficient Memory Management for Large Language Model Serving with PagedAttention , 2023`
   - Bu makale, büyük dil modellerinin sunumu için Sayfalandırılmış Dikkat (PagedAttention) kullanarak etkin bellek yönetimini tartışmaktadır.
   - PagedAttention, bellek kullanımını optimize eden bir tekniktir.

3. `# Nvidia, TensorRT-LLM , https://github.com/NVIDIA/TensorRT-LLM , 2023`
   - Nvidia tarafından geliştirilen TensorRT-LLM projesine atıfta bulunmaktadır.
   - Bu proje, büyük dil modellerinin çıkarımını hızlandırmaya yönelik optimize edilmiş bir çerçeve sunar.

4. `# Y. Leviathan, M. Kalman, Y. Matias, Fast Inference from Transformers via Speculative Decoding, 2023`
   - Bu makale, Spekülatif Kod Çözme (Speculative Decoding) yoluyla transformatörlerden hızlı çıkarım yapmayı önerir.
   - Spekülatif Kod Çözme, çıkarım sürecini hızlandırmak için kullanılan bir tekniktir.

5. `# T. Cai, Y. Li, Z. Geng, H. Peng, J.D. Lee, D. Chen, T. Dao, Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads, 2024`
   - Bu makale, Medusa adlı basit bir Büyük Dil Modeli çıkarım hızlandırma çerçevesini tanıtır.
   - Medusa, çoklu kod çözme başlıkları kullanarak çıkarımı hızlandırır.

6. `# R.Y. Aminabadi, S. Rajbhandari, M. Zhang, A.A. Awan, C. Li, D. Li, E. Zheng, J. Rasley, S. Smith, O. Ruwase, Y. He, DeepSpeed Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale, 2022`
   - Bu makale, DeepSpeed Çıkarımı'nı tanıtır ve transformatör modellerinin eşi görülmemiş ölçekte etkin çıkarımını sağlar.
   - DeepSpeed, büyük ölçekli modeller için optimize edilmiş bir çıkarım motorudur.

7. `# Y. Huang, Y. Cheng, A. Bapna, O. Firat, M.X. Chen, D. Chen, H. Lee, J. Ngiam, Q.V. Le, Y. Wu, Z. Chen, GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism, 2019`
   - Bu makale, GPipe'ı tanıtır ve boru hattı paralelliği kullanarak dev sinir ağlarının etkin eğitilmesini sağlar.
   - GPipe, büyük modellerin eğitimini hızlandırmak için boru hattı paralelliği kullanır.

8. `# K. James Reed, PiPPy: Pipeline Parallelism for PyTorch , https://github.com/pytorch/PiPPy , 2022`
   - PyTorch için PiPPy adlı boru hattı paralelliği projesine atıfta bulunmaktadır.
   - PiPPy, PyTorch modellerinde boru hattı paralelliği sağlar.

9. `# M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, B. Catanzaro, Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism, 2020`
   - Bu makale, Megatron-LM'i tanıtır ve model paralelliği kullanarak çoklu-milyar parametreli dil modellerinin eğitilmesini sağlar.
   - Megatron-LM, büyük dil modellerinin eğitimi için model paralelliği kullanır.

10. `# Verma ve Vaidya, Mastering LLM Techniques: Inference Optimization, NVIDIA Developer Technical Blog , https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/ , 2023`
    - Bu blog yazısı, büyük dil modeli tekniklerinde ustalaşma ve çıkarım optimizasyonu hakkında bilgi verir.
    - NVIDIA'nın geliştirdiği teknikler ve öneriler içerir.

11. `# T. Dettmers, M. Lewis, Y. Belkada, L. Zettlemoyer, LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale, 2022`
    - Bu makale, LLM.int8() adlı 8-bit matris çarpımı tekniğini tanıtır.
    - Bu teknik, transformatörlerde ölçekte etkin matris çarpımı sağlar.

12. `# G. Gerganov, llama.cpp , https://github.com/ggerganov/llama.cpp , 2023`
    - Llama.cpp projesine atıfta bulunmaktadır.
    - Bu proje, C++ tabanlı bir büyük dil modeli çıkarım çerçevesidir.

13. `# E. Frantar, S. Ashkboos, T. Hoefler, D. Alistarh, GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers, 2023`
    - Bu makale, GPTQ adlı doğru eğitim sonrası kuantalama tekniğini tanıtır.
    - GPTQ, üretken önceden eğitilmiş transformatörler için kullanılır.

14. `# Tuboderp, exllamav2 , https://github.com/turboderp/exllamav2 , 2023`
    - Exllamav2 projesine atıfta bulunmaktadır.
    - Bu proje, büyük dil modellerinin çıkarımı için optimize edilmiş bir çerçeve sunar.

15. `# J. Lin, J. Tang, H. Tang, S. Yang, W.-M. Chen, W.-C. Wang, G. Xiao, X. Dang, C. Gan, S. Han, AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration, 2024`
    - Bu makale, AWQ adlı aktivasyon-duyarlı ağırlık kuantalama tekniğini tanıtır.
    - AWQ, büyük dil modeli sıkıştırma ve hızlandırma için kullanılır.

Bu kaynaklar, büyük dil modellerinin çıkarımını optimize etmeye yönelik çeşitli teknikler ve projeler hakkında bilgi sağlar.

---

