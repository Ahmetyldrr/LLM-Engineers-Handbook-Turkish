# Referanslar

Tahori, Gulrajani, Zhang, Dubois, et al.. “ Alpaca: A Strong, Replicable Instruction-Following Model (Alpaca: Güçlü ve Tekrarlanabilir Talimat Takip Modeli)” crfm.stanford.edu , March 13, 2023, https://crfm.stanford.edu/2023/03/13/alpaca.html .

Subhabrata Mukherjee et al.. “ Orca: Progressive Learning from Complex Explanation Traces of GPT-4 (Orca: GPT-4'ün Karmaşık Açıklama İzlerinden Progresif Öğrenme)” arXiv preprint arXiv:2306.02707, June 2023.

Wing Lian and Bleys Goodson and Eugene Pentland and Austin Cook and Chanvichet Vong and “Teknium”. “ Open-Orca/OpenOrca (Açık-Orca/AçıkOrca).” huggingface.co , 2023, https://huggingface.co/datasets/Open-Orca/OpenOrca .

Weihao Zeng et al.. “ Automatic Instruction Evolving for Large Language Models (Büyük Dil Modelleri için Otomatik Talimat Evrimi)” arXiv preprint arXiv:2406.00770, June 2024.

Chunting Zhou et al.. “ LIMA: Less Is More for Alignment (LIMA: Hizalama için Daha Az Daha Fazladır)” arXiv preprint arXiv:2305.11206, May 2023.

01. AI. “ Yi: Open Foundation Models by 01.AI (Yi: 01.AI tarafından Açık Temel Modelleri)” arXiv preprint arXiv:2403.04652, March 2024.

Alex Birch. “ LLM finetuning memory requirements (LLM İnce Ayar Bellek Gereksinimleri)” blog.scottlogic.com , November 24, 2023, https://blog.scottlogic.com/2023/11/24/llm-mem.html .

Quentin Anthony et al.. “ Transformer Math 101 (Transformer Matematik 101)” blog.eleuther.ai, April 18, 2023, https://blog.eleuther.ai/transformer-math/ .

Edward J. Hu et al.. “ LoRA: Low-Rank Adaptation of Large Language Models (LoRA: Büyük Dil Modellerinin Düşük Dereceli Uyarlaması)” arXiv preprint arXiv:2106.09685, June 2021.

Tim Dettmers et al.. “ QLoRA: Efficient Finetuning of Quantized LLMs (QLoRA: Nicelenmiş LLM'lerin Verimli İnce Ayarı)” arXiv preprint arXiv:2305.14314, May 2023.

Kodların ayrıntılı açıklaması:

1. `Tahori, Gulrajani, Zhang, Dubois, et al.. “ Alpaca: A Strong, Replicable Instruction-Following Model ”`
   - Bu satır, "Alpaca: A Strong, Replicable Instruction-Following Model" başlıklı makalenin yazarlarını ve başlığını temsil etmektedir.
   - Yazarlar: Tahori, Gulrajani, Zhang, Dubois, et al. (ve diğerleri)
   - Başlık: Alpaca: A Strong, Replicable Instruction-Following Model (Alpaca: Güçlü ve Tekrarlanabilir Talimat Takip Modeli)

2. `crfm.stanford.edu , March 13, 2023, https://crfm.stanford.edu/2023/03/13/alpaca.html`
   - Bu satır, makalenin yayınlandığı tarih ve URL'sini temsil etmektedir.
   - Yayın tarihi: 13 Mart 2023
   - URL: https://crfm.stanford.edu/2023/03/13/alpaca.html

Diğer satırlar da benzer şekilde açıklanabilir.

İngilizce teknik terimlerin Türkçeleri parantez içinde eklenmiştir.

Kodların birebir aynısı aşağıdaki gibidir:

```
Tahori, Gulrajani, Zhang, Dubois, et al.. “ Alpaca: A Strong, Replicable Instruction-Following Model (Alpaca: Güçlü ve Tekrarlanabilir Talimat Takip Modeli)” crfm.stanford.edu , March 13, 2023, https://crfm.stanford.edu/2023/03/13/alpaca.html .
Subhabrata Mukherjee et al.. “ Orca: Progressive Learning from Complex Explanation Traces of GPT-4 (Orca: GPT-4'ün Karmaşık Açıklama İzlerinden Progresif Öğrenme)” arXiv preprint arXiv:2306.02707, June 2023.
Wing Lian and Bleys Goodson and Eugene Pentland and Austin Cook and Chanvichet Vong and “Teknium”. “ Open-Orca/OpenOrca (Açık-Orca/AçıkOrca).” huggingface.co , 2023, https://huggingface.co/datasets/Open-Orca/OpenOrca .
Weihao Zeng et al.. “ Automatic Instruction Evolving for Large Language Models (Büyük Dil Modelleri için Otomatik Talimat Evrimi)” arXiv preprint arXiv:2406.00770, June 2024.
Chunting Zhou et al.. “ LIMA: Less Is More for Alignment (LIMA: Hizalama için Daha Az Daha Fazladır)” arXiv preprint arXiv:2305.11206, May 2023.
01. AI. “ Yi: Open Foundation Models by 01.AI (Yi: 01.AI tarafından Açık Temel Modelleri)” arXiv preprint arXiv:2403.04652, March 2024.
Alex Birch. “ LLM finetuning memory requirements (LLM İnce Ayar Bellek Gereksinimleri)” blog.scottlogic.com , November 24, 2023, https://blog.scottlogic.com/2023/11/24/llm-mem.html .
Quentin Anthony et al.. “ Transformer Math 101 (Transformer Matematik 101)” blog.eleuther.ai, April 18, 2023, https://blog.eleuther.ai/transformer-math/ .
Edward J. Hu et al.. “ LoRA: Low-Rank Adaptation of Large Language Models (LoRA: Büyük Dil Modellerinin Düşük Dereceli Uyarlaması)” arXiv preprint arXiv:2106.09685, June 2021.
Tim Dettmers et al.. “ QLoRA: Efficient Finetuning of Quantized LLMs (QLoRA: Nicelenmiş LLM'lerin Verimli İnce Ayarı)” arXiv preprint arXiv:2305.14314, May 2023.
```

---

